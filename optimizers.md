# Learning rates scheduling & optimizers

### 2017:

- [X] **Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates**,
Smith et al. [`arxiv`](https://arxiv.org/abs/1708.07120)
- [X] **FreezeOut: Accelerate Training by Progressively Freezing Layers**, Brock et al.
[`arxiv`](https://arxiv.org/abs/1706.04983)

### 2016:

- [X] **An overview of gradient descent optimization algorithms**, Ruder.
[`arxiv`](https://arxiv.org/abs/1609.04747) :star: