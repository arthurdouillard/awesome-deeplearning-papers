# Ensembling & distillation

### 2018:

- [X] **Averaging Weights Leads to Wider Optima and Better Generalization**, Izmailov et al.
[`arxiv`](https://arxiv.org/abs/1803.05407) :star:
- [X] **Born Again Neural Networks**, Furlanello et al. [`arxiv`](https://arxiv.org/abs/1805.04770)
- [X] **Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs**,
Garipov et al. [`arxiv`](https://arxiv.org/abs/1802.10026)

### 2017:

- [X] **Snapshot Ensembles: Train 1, get M for free**, Huang et al.
[`arxiv`](https://arxiv.org/abs/1704.00109)

### 2015:

- [X] **Distilling the Knowledge in a Neural Network**, Hinton et al.
[`arxiv`](https://arxiv.org/abs/1503.02531) :star: