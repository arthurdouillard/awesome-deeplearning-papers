# Ensembling / distillation / compression / pruning

### 2018:

- [X] **The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks**, Frankle and Carbin.
[`arxiv`](https://arxiv.org/abs/1803.03635)
- [X] **Averaging Weights Leads to Wider Optima and Better Generalization**, Izmailov et al.
[`arxiv`](https://arxiv.org/abs/1803.05407) :star:
- [X] **Born Again Neural Networks**, Furlanello et al. [`arxiv`](https://arxiv.org/abs/1805.04770)
- [X] **Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs**,
Garipov et al. [`arxiv`](https://arxiv.org/abs/1802.10026)

### 2017:

- [X] **Snapshot Ensembles: Train 1, get M for free**, Huang et al.
[`arxiv`](https://arxiv.org/abs/1704.00109)

### 2015:

- [ ] **Learning both Weights and Connections for Efficient Neural Networks**,
Han et al. [`arxiv`](https://arxiv.org/abs/1506.02626)
- [X] **Distilling the Knowledge in a Neural Network**, Hinton et al.
[`arxiv`](https://arxiv.org/abs/1503.02531) :star:

### 2014:

- [ ] **https://arxiv.org/abs/1412.6550**, Romera et al.
[`arxiv`](https://arxiv.org/abs/1412.6550)