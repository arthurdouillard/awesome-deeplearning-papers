# Miscealleneous

### 2018:

- [ ] **Dropout is a special case of the stochastic delta rule: faster and more accurate deep learning**, Frazier-Logue and Hanson. [`arxiv`](https://arxiv.org/abs/1808.03578v1)
- [X] **Why do deep convolutional networks generalize so poorly to small image transformations?**,
Azulay et al. [`arxiv`](https://arxiv.org/abs/1805.12177) :star:
- [X] **Pooling is neither necessary nor sufficient for appropriate deformation stability in CNNs**,
Ruderman et al. [`arxiv`](https://arxiv.org/abs/1804.04438)

### 2017:

- [ ] **One pixel attack for fooling deep neural networks**, Su et al.
[`arxiv`](https://arxiv.org/abs/1710.08864)
- [X] **Gradual Tuning: a better way of Fine Tuning the parameters of a Deep Neural Network**,
Montone et al. [`arxiv`](https://arxiv.org/abs/1711.10177)
- [X] **Deep Learning Scaling is Predictable, Empirically**, Hestness et al.
[`arxiv`](https://arxiv.org/abs/1712.00409) :star:

### 2016:

- [X] **Systematic evaluation of CNN advances on the ImageNet**, Mishkin et al.
[`arxiv`](https://arxiv.org/abs/1606.02228)

### 2015:

- [X] **Empirical Evaluation of Rectified Activations in Convolutional Network**,
Xu et al. [`arxiv`](https://arxiv.org/abs/1505.00853)
- [ ] **The Multiverse Loss for Robust Transfer Learning**, Littwin and Wolf.
[`arxiv`](https://arxiv.org/abs/1511.09033)
- [ ] **Spatial Transformer Networks**, Jaderberg et al.
[`arxiv`](https://arxiv.org/abs/1506.02025)

### 2014:

- [X] **How transferable are features in deep neural networks?**, Yosinski et al.
[`url`](http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks)
- [X] **Unsupervised Domain Adaptation by Backpropagation**, Ganin and Lempitsky.
[`arxiv`](https://arxiv.org/abs/1409.7495) :star:

### 2012:

- [X] **A Few Useful Things to Know about Machine Learning**, Domingos.
[`pdf`](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)