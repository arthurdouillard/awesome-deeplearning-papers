# Recurrent Neural Networks architecture

### 2018:

- [ ] **Sliced Recurrent Neural Networks**, Yu and Liu. [`arxiv`](https://arxiv.org/abs/1807.02291)
- [ ] **QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension**,
Wei Yu et al. [`arxiv`](https://arxiv.org/abs/1804.09541)

### 2017:

- [ ] **FusionNet: Fusing via Fully-Aware Attention with Application to Machine Comprehension**,
Huang et al. [`arxiv`](https://arxiv.org/abs/1711.07341)

### 2016:

- [ ] **Text Understanding with the Attention Sum Reader Network**, Kadlec et al.
[`arxiv`](https://arxiv.org/abs/1603.01547)
- [ ] **Bidirectional Attention Flow for Machine Comprehension**, Seo et al.
[`arxiv`](https://arxiv.org/abs/1611.01603)

### 2015:

- [ ] **Pointer Networks**, Vinyals et al. [`arxiv`](https://arxiv.org/abs/1506.03134)
