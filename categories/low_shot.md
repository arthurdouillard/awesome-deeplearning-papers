# Zero-shot, one-shot, and low-shot papers

### 2018:

- [X] **Optimization as a model for few-shots learning**, Ravi and Larochelle. [`pdf`](https://openreview.net/pdf?id=rJY0-Kcll)
- [ ] **Few-shot learning of neural networks from scratch by pseudo example optimization**, Kimura et al. [`arxiv`](https://arxiv.org/abs/1802.03039)
- [ ] **Meta-Learning for Semi-Supervised Few-Shot Classification**, Ren et al. [`arxiv`](https://arxiv.org/abs/1803.00676)
- [X] **Deep Triplet Ranking Networks for One-Shot Recognition**, Ye and Guo.
[`arxiv`](https://arxiv.org/abs/1804.07275v1)

### 2017:

- [ ] **One-Shot Imitation Learning**, Duan et al. [`arxiv`](https://arxiv.org/abs/1703.07326)
- [ ] **Generative Adversarial Residual Pairwise Networks for One Shot Learning**, Mehrotra et al.
[`arxiv`](https://arxiv.org/abs/1703.08033v1)
- [X] **Prototypical Networks for Few-shot Learning**, Snell et al.
[`arxiv`](https://arxiv.org/abs/1703.05175)
- [X] **Beyond triplet loss: a deep quadruplet network for person re-identification**,
Chen et al. [`arxiv`](https://arxiv.org/abs/1704.01719)

### 2016:

- [X] **Matching Networks for One Shot Learning**, Vinyals et al. [`arxiv`](https://arxiv.org/abs/1606.04080) :star:
- [ ] **Low-shot Visual Recognition by Shrinking and Hallucinating Features**, Hariharan et al. [`arxiv`](https://arxiv.org/abs/1606.02819)
- [ ] **One-shot Learning with Memory-Augmented Neural Networks**, Santoro et al. [`arxiv`](https://arxiv.org/abs/1605.06065)
- [ ] **LCNN: Lookup-based Convolutional Neural Network**, Bagherinezhad et al.
[`arxiv`](https://arxiv.org/abs/1611.06473)
- [ ] **Learning Deep Embeddings with Histogram Loss**, Ustinova and Lempitsky.
[`arxiv`](https://arxiv.org/abs/1611.00822)

### 2015:

- [X] **Siamese Neural Networks for One-shot Image Recognition**, Koch et al. [`pdf`](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf) :star:
- [X] **Deep metric learning using Triplet network**, Hoffer et al. [`arxiv`](https://arxiv.org/abs/1412.6622)

### Pre-2015:

- [ ] **Zero-Shot Learning Through Cross-Modal Transfer**, Socher et al., 2013. [`arxiv`](https://arxiv.org/abs/1301.3666)


# Datasets

- **Omniglot**. [`github`](https://github.com/brendenlake/omniglot)
  - 1623 different handwritten characters written by 20 different persons.
- **UCSD Bird**. [`url`](http://www.vision.caltech.edu/visipedia/CUB-200.html)
  - 6000 images of 200 birds (30 images per bird).
